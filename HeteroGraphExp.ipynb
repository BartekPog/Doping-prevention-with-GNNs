{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from itertools import cycle\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch_geometric.transforms import RandomLinkSplit, RandomNodeSplit\n",
    "\n",
    "from src.dmgi import DMGI\n",
    "from src.data_preparation import DataPreprocessor\n",
    "from src.graph_data_loader import graph_loader,  heterogeneous_graph_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "het_train_graph, het_test_graph = heterogeneous_graph_loader(split_type='whole', swap_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "het_transform = RandomNodeSplit(num_train_per_class=400)\n",
    "het_data = het_transform(het_train_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DMGI(het_data['sample'].num_nodes, het_data['sample'].x.size(-1),\n",
    "             out_channels=128, num_relations=len(het_data.edge_types))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "het_data, model = het_data.to(device), model.to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.005, weight_decay=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x = het_data['sample'].x\n",
    "    edge_indices = het_data.edge_index_dict.values()\n",
    "    pos_hs, neg_hs, summaries = model(x, edge_indices)\n",
    "    loss = model.loss(pos_hs, neg_hs, summaries)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    train_emb = model.Z[het_data['sample'].train_mask].cpu()\n",
    "    val_emb = model.Z[het_data['sample'].val_mask].cpu()\n",
    "    test_emb = model.Z[het_data['sample'].test_mask].cpu()\n",
    "\n",
    "    train_y = het_data['sample'].y[het_data['sample'].train_mask].cpu()\n",
    "    val_y = het_data['sample'].y[het_data['sample'].val_mask].cpu()\n",
    "    test_y = het_data['sample'].y[het_data['sample'].test_mask].cpu()\n",
    "\n",
    "    #clf = LogisticRegression(class_weight='balanced').fit(train_emb, train_y)\n",
    "    clf = make_pipeline(StandardScaler(), LogisticRegression(class_weight='balanced'))\n",
    "    clf.fit(train_emb, train_y)\n",
    "    print(clf.classes_)\n",
    "    train_score = metrics.roc_auc_score(train_y, clf.predict(train_emb))\n",
    "    val_score = metrics.roc_auc_score(val_y, clf.predict(val_emb))\n",
    "    test_score = metrics.roc_auc_score(test_y, clf.predict(test_emb))\n",
    "    return train_score, val_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "Epoch: 001, Loss: 17967978.0000, Train: 0.5840, Val: 0.4714, Test: 0.4871\n",
      "[0 1]\n",
      "Epoch: 002, Loss: 17895790.0000, Train: 0.5863, Val: 0.4770, Test: 0.4931\n",
      "[0 1]\n",
      "Epoch: 003, Loss: 12689805.0000, Train: 0.5869, Val: 0.4896, Test: 0.5018\n",
      "[0 1]\n",
      "Epoch: 004, Loss: 6611029.5000, Train: 0.5918, Val: 0.4730, Test: 0.5170\n",
      "[0 1]\n",
      "Epoch: 005, Loss: 12906369.0000, Train: 0.5883, Val: 0.4730, Test: 0.5181\n",
      "[0 1]\n",
      "Epoch: 006, Loss: 5122281.0000, Train: 0.5895, Val: 0.4856, Test: 0.5002\n",
      "[0 1]\n",
      "Epoch: 007, Loss: 10422079.0000, Train: 0.5926, Val: 0.4733, Test: 0.5063\n",
      "[0 1]\n",
      "Epoch: 008, Loss: 4420702.0000, Train: 0.5895, Val: 0.4643, Test: 0.5005\n",
      "[0 1]\n",
      "Epoch: 009, Loss: 1632308.8750, Train: 0.5906, Val: 0.4669, Test: 0.5041\n",
      "[0 1]\n",
      "Epoch: 010, Loss: -806072.9375, Train: 0.5897, Val: 0.4461, Test: 0.5151\n",
      "[0 1]\n",
      "Epoch: 011, Loss: 2053672.5000, Train: 0.5875, Val: 0.4461, Test: 0.5106\n",
      "[0 1]\n",
      "Epoch: 012, Loss: 1098337.8750, Train: 0.5876, Val: 0.4542, Test: 0.5177\n",
      "[0 1]\n",
      "Epoch: 013, Loss: -3900273.5000, Train: 0.5849, Val: 0.4610, Test: 0.5202\n",
      "[0 1]\n",
      "Epoch: 014, Loss: -5052429.0000, Train: 0.5878, Val: 0.4576, Test: 0.5260\n",
      "[0 1]\n",
      "Epoch: 015, Loss: -4018140.0000, Train: 0.5838, Val: 0.4554, Test: 0.5271\n",
      "[0 1]\n",
      "Epoch: 016, Loss: -6841496.5000, Train: 0.5856, Val: 0.4565, Test: 0.5319\n",
      "[0 1]\n",
      "Epoch: 017, Loss: -6292665.5000, Train: 0.5846, Val: 0.4461, Test: 0.5198\n",
      "[0 1]\n",
      "Epoch: 018, Loss: -7932343.5000, Train: 0.5856, Val: 0.4427, Test: 0.5215\n",
      "[0 1]\n",
      "Epoch: 019, Loss: -6306608.0000, Train: 0.5857, Val: 0.4357, Test: 0.5058\n",
      "[0 1]\n",
      "Epoch: 020, Loss: -8650323.0000, Train: 0.5879, Val: 0.4242, Test: 0.5127\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 21):\n",
    "    loss = train()\n",
    "    if epoch % 1 == 0:\n",
    "        train_acc, val_acc, test_acc = test()\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "              f'Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9a0203e2f48670f0925371d1cf6517870bac1f0fcff9fababa230439f0ab4d72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
